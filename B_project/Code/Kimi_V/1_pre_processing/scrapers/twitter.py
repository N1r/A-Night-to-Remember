"""
twitter.py
----------
X (Twitter) è§†é¢‘é‡‡é›†å™¨ã€‚

é€šè¿‡ twikit åº“æŠ“å–æŒ‡å®šåšä¸»çš„æ¨æ–‡ä¸­çš„è§†é¢‘ï¼Œ
æ”¯æŒåŸåˆ›/è½¬å‘/å¼•ç”¨ä¸‰ç§æ¥æºã€‚

é…ç½®æ¥æºï¼š
  - è´¦å·åˆ—è¡¨ï¼šconfigs/domains/<domain>.yaml â†’ scrapers.twitter.accounts
  - Cookie è·¯å¾„ï¼š1_pre_processing/scrapers/twitter_cookies.json
"""

import asyncio
import json
import os
import random
import sys
import platform
from pathlib import Path
from typing import List, Dict

PROJECT_ROOT = Path(__file__).parent.parent.parent.absolute()
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from twikit import Client
from core.utils.config_utils import load_key
from shared.paths import TASKS_EXCEL, PRE_PROCESSING
from shared.domain import domain
from _base_scraper import BaseScraper


# ä»é¢†åŸŸé…ç½®è¯»å–è´¦å·åˆ—è¡¨
def _get_accounts() -> list:
    accounts = domain.get("scrapers.twitter.accounts", [])
    return accounts if accounts else []


# Cookie æ–‡ä»¶è·¯å¾„
COOKIES_PATH = PRE_PROCESSING / "scrapers" / "twitter_cookies.json"
if not COOKIES_PATH.exists():
    COOKIES_PATH = PRE_PROCESSING / "scrapers" / "cookies.json"


def _get_val(obj, key, default=None):
    if obj is None:
        return default
    if isinstance(obj, dict):
        return obj.get(key, default)
    return getattr(obj, key, default)


def _parse_counts(val):
    if not val:
        return 0
    if isinstance(val, int):
        return val
    val = str(val).upper().replace(",", "").strip()
    try:
        if "K" in val:
            return int(float(val.replace("K", "")) * 1000)
        if "M" in val:
            return int(float(val.replace("M", "")) * 1000000)
        return int(float(val))
    except Exception:
        return 0


class TwitterScraper(BaseScraper):
    name = "X (Twitter)"

    def __init__(self):
        super().__init__()
        self.client = Client("en-US")
        self._load_cookies()

    def _load_cookies(self):
        """åŠ è½½ Twitter Cookieï¼ˆå…¼å®¹ Playwright æ ¼å¼å’Œ twikit æ ¼å¼ï¼‰"""
        if not COOKIES_PATH.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° Cookie æ–‡ä»¶: {COOKIES_PATH}")

        try:
            self.client.load_cookies(str(COOKIES_PATH))
        except Exception:
            # Playwright æ ¼å¼è½¬ twikit æ ¼å¼
            with open(COOKIES_PATH, "r", encoding="utf-8") as f:
                playwright_cookies = json.load(f)
            twikit_cookies = {c["name"]: c["value"] for c in playwright_cookies}
            temp_path = str(COOKIES_PATH) + ".temp.json"
            with open(temp_path, "w", encoding="utf-8") as f:
                json.dump(twikit_cookies, f)
            try:
                self.client.load_cookies(temp_path)
            finally:
                # æ— è®ºæˆåŠŸæˆ–å¼‚å¸¸ï¼Œå‡ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                if os.path.exists(temp_path):
                    os.remove(temp_path)

    async def _fetch_account(self, screen_name: str, count: int = 20) -> List[Dict]:
        """æŠ“å–å•ä¸ªè´¦å·çš„è§†é¢‘"""
        results = []
        try:
            from shared.logger import console
            console.print(f"    [dim]ğŸ” æ­£åœ¨æŠ“å– X ç”¨æˆ·: @{screen_name}...[/dim]")
            await asyncio.sleep(random.uniform(1, 2))
            user = await self.client.get_user_by_screen_name(screen_name)

            # æ··åˆæŠ“å– Tweets + Media
            all_tweets = []
            for tab in ["Tweets", "Media"]:
                try:
                    # console.print(f"      [dim]æŠ“å– {tab} æ ‡ç­¾é¡µ...[/dim]")
                    ts = await user.get_tweets(tab, count=count)
                    if ts:
                        all_tweets.extend(ts)
                    await asyncio.sleep(1)
                except Exception:
                    pass
            
            # console.print(f"      [dim]è·å–åˆ° {len(all_tweets)} æ¡æ¨æ–‡ï¼Œæ­£åœ¨å»é‡...[/dim]")

            # å»é‡æ’åº
            unique = sorted(
                {t.id: t for t in all_tweets}.values(),
                key=lambda x: _get_val(x, "created_at", ""),
                reverse=True,
            )
            
            # console.print(f"      [dim]å»é‡åå‰©ä½™ {len(unique)} æ¡ï¼Œå¼€å§‹è§£æè§†é¢‘...[/dim]")

            for tweet in unique:
                created_at = _get_val(tweet, "created_at", "unknown")
                is_retweet = hasattr(tweet, "retweeted_tweet") and tweet.retweeted_tweet
                is_quote = hasattr(tweet, "quoted_tweet") and tweet.quoted_tweet

                target_tweet = tweet
                source_type = "åŸåˆ›"
                if is_retweet:
                    target_tweet = tweet.retweeted_tweet
                    source_type = "è½¬å‘"
                elif is_quote and not (hasattr(tweet, "media") and tweet.media):
                    target_tweet = tweet.quoted_tweet
                    source_type = "å¼•ç”¨"

                media = _get_val(target_tweet, "media")
                if not media:
                    continue

                video_url = None
                v_info = {}
                for media_item in media:
                    m_type = _get_val(media_item, "type")
                    if m_type in ["video", "animated_gif"]:
                        v_info = _get_val(media_item, "video_info") or media_item
                        variants = _get_val(v_info, "variants", [])
                        # è¿‡æ»¤æ‰ bitrate=0 çš„ HTTP æµå˜ä½“ï¼ˆm3u8 ç­‰ï¼‰ï¼Œåªä¿ç•™ç›´é“¾ MP4
                        valid = [
                            {"url": _get_val(v, "url"), "bitrate": _get_val(v, "bitrate", 0)}
                            for v in variants
                            if _get_val(v, "url") and _get_val(v, "bitrate", 0) > 0
                        ]
                        # è‹¥è¿‡æ»¤åä¸ºç©ºï¼ˆå…¨ä¸º HTTP æµï¼‰ï¼Œé™çº§ä½¿ç”¨ä»»ä½•å¯ç”¨ URL
                        if not valid:
                            valid = [
                                {"url": _get_val(v, "url"), "bitrate": 0}
                                for v in variants if _get_val(v, "url")
                            ]
                        if valid:
                            video_url = max(valid, key=lambda x: x["bitrate"])["url"]
                            break

                if not video_url or not self.is_new(video_url):
                    continue

                duration_ms = _get_val(v_info, "duration_ms", 0)
                # å°è¯•å¤šä¸ªæ¥æºè·å–æ—¶é•¿
                if duration_ms:
                    duration_sec = duration_ms / 1000
                else:
                    # å°è¯•ä» tweet å¯¹è±¡çš„å…¶ä»–å­—æ®µè·å–
                    # æœ‰æ—¶ twikit å°† duration ç›´æ¥æ”¾åœ¨ media å¯¹è±¡ä¸­
                    duration_ms_alt = _get_val(media_item, "duration_ms", 0)
                    if duration_ms_alt:
                        duration_sec = duration_ms_alt / 1000
                    else:
                        duration_sec = _get_val(target_tweet, "duration", 0)

                # å¦‚æœä»ç„¶è·å–ä¸åˆ°æ—¶é•¿ï¼Œè®©å…¶ä¿æŒä¸º 0ï¼Œä»¥ä¾¿ workflow_1_pre.py è°ƒç”¨ yt-dlp è¡¥å…¨
                # ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç›´æ¥ä½¿ç”¨ video_url (mp4ç›´é“¾) å¯èƒ½æ— æ³•é€šè¿‡ yt-dlp è·å–æ—¶é•¿
                # æ›´å¥½çš„æ–¹å¼æ˜¯ä¿å­˜åŸå§‹æ¨æ–‡é“¾æ¥
                
                # æ„é€ åŸå§‹æ¨æ–‡é“¾æ¥
                tweet_url = f"https://x.com/{screen_name}/status/{target_tweet.id}"
                
                # å†…å®¹é¢„ç­›é€‰
                title_text = _get_val(target_tweet, "text", "").replace("\n", " ")
                if not self.validate_content(title_text):
                    continue

                entry = self.make_entry(
                    title=title_text[:100],
                    rawtext=title_text,
                    duration=duration_sec,
                    viewCount=_parse_counts(_get_val(target_tweet, "view_count", 0)),
                    Reposts=_parse_counts(_get_val(target_tweet, "retweet_count", 0)),
                    channel_name=f"X_{screen_name}",
                    **{
                        "Video File": video_url, # è¿™é‡Œä»ç„¶ä¿å­˜ MP4 ç›´é“¾ä¾›ä¸‹è½½ä½¿ç”¨
                        "Original URL": tweet_url, # ä¿å­˜åŸå§‹é“¾æ¥ç”¨äºå…ƒæ•°æ®è¡¥å…¨
                        "Publish Date": str(created_at),
                        "Target Language": load_key("target_language") or "ç®€ä½“ä¸­æ–‡",
                    },
                )
                entry["source_type"] = source_type
                results.append(entry)
                # console.print(f"      [dim green]+ å‘ç°è§†é¢‘: {entry['title'][:20]}...[/dim green]")

            if results:
                console.print(f"    [dim green]@{screen_name}: é‡‡é›†åˆ° {len(results)} ä¸ªæ–°è§†é¢‘[/dim green]")
            else:
                console.print(f"    [dim]@{screen_name}: æœªå‘ç°æ–°è§†é¢‘[/dim]")

        except Exception as e:
            from shared.logger import console
            console.print(f"  [red]è·å– @{screen_name} å¤±è´¥: {e}[/red]")

        return results

    async def _fetch_async(self) -> List[Dict]:
        from shared.logger import create_progress
        accounts = _get_accounts()
        if not accounts:
            return []
        all_videos = []
        with create_progress() as progress:
            task = progress.add_task("[cyan]é‡‡é›† X æ•°æ®...", total=len(accounts))
            for screen_name in accounts:
                progress.update(task, description=f"ğŸ” å¤„ç† X: @{screen_name}")
                videos = await self._fetch_account(screen_name, count=20)
                all_videos.extend(videos)
                progress.advance(task)
        return all_videos

    def fetch(self) -> List[Dict]:
        if platform.system() == "Windows":
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        return asyncio.run(self._fetch_async())


# ä¿æŒæ—§æ¥å£å…¼å®¹
def fetch_X_main():
    return TwitterScraper().run()


if __name__ == "__main__":
    res = fetch_X_main()
    print(f"Captured {len(res)} videos.")
