"""
workflow_1_pre.py
-----------------
é˜¶æ®µ 1ï¼šæ™ºèƒ½é‡‡é›†å·¥ä½œæµã€‚

å…¨è‡ªåŠ¨æ‰§è¡Œï¼š
  1. å¤šå¹³å°å¹¶å‘é‡‡é›†ï¼ˆYouTube / X / Blueskyï¼‰
  2. å¢é‡è¿‡æ»¤ï¼ˆè·³è¿‡å·²è®¿é—®è§†é¢‘ï¼‰
  3. AI å¹¶å‘ç­›é€‰ï¼ˆæ‰“åˆ† + åˆ†ç±» + ä¸­æ–‡æ ‡é¢˜ç”Ÿæˆï¼‰
  4. åˆå¹¶å†™å…¥ä»»åŠ¡åˆ—è¡¨ Excel

è¿è¡Œæ–¹å¼ï¼š
    python 1_pre_processing/workflow_1_pre.py
"""

import os
import sys
import importlib
import pandas as pd
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from yt_dlp import YoutubeDL

# ==================== è·¯å¾„è®¾ç½® ====================
PROJECT_ROOT = Path(__file__).parent.parent.absolute()
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# ç¡®ä¿ 2_mid_processing è·¯å¾„å¯ç”¨ï¼ˆconfig_utils ä¾èµ–æ‰€åœ¨ï¼‰
sys.path.append(str(PROJECT_ROOT / "2_mid_processing"))
sys.path.append(str(Path(__file__).parent / "scrapers"))

from shared.paths import TASKS_EXCEL, VISITED_LOG
from shared.state import load_visited, save_visited
from shared.logger import console, create_progress, Panel, Table
from core.utils.config_utils import load_key
from core.utils.ask_gpt import ask_gpt

# ==================== æ•°æ®æ ¼å¼ ====================

from _base_scraper import STANDARD_COLUMNS

# ==================== é‡‡é›†å™¨æ³¨å†Œè¡¨ ====================

SCRAPERS = [
    "youtube",
    "twitter",
    "bluesky",
]


def _load_scrapers():
    """åŠ¨æ€åŠ è½½æ‰€æœ‰é‡‡é›†å™¨æ¨¡å—"""
    loaded = []
    for name in SCRAPERS:
        try:
            mod = importlib.import_module(name)
            # æ‰¾åˆ°ç»§æ‰¿ BaseScraper çš„ç±»
            from _base_scraper import BaseScraper
            for attr_name in dir(mod):
                attr = getattr(mod, attr_name)
                if (isinstance(attr, type) and issubclass(attr, BaseScraper)
                        and attr is not BaseScraper):
                    loaded.append(attr())
                    break
        except Exception as e:
            console.print(f"[red]âŒ åŠ è½½é‡‡é›†å™¨ {name} å¤±è´¥: {e}[/red]")
    return loaded

def get_video_duration(url: str) -> float:
    """ä½¿ç”¨ yt-dlp è·å–è§†é¢‘æ—¶é•¿ï¼ˆä¸ä¸‹è½½è§†é¢‘ï¼‰"""
    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'simulate': True,
        'skip_download': True,
        'extract_flat': 'in_playlist', # å°½å¯èƒ½å¿«åœ°æå–ä¿¡æ¯
        'socket_timeout': 10,          # 10ç§’è¿æ¥è¶…æ—¶
        'retries': 3,                  # é‡è¯•3æ¬¡
    }
    try:
        with YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
            return float(info.get('duration', 0))
    except Exception as e:
        # console.print(f"[dim yellow]âš ï¸ æ— æ³•è·å–æ—¶é•¿: {url} ({e})[/dim yellow]")
        return 0.0

# ==================== AI ç­›é€‰ ====================

def _evaluate_single(v: dict) -> dict:
    """AI è¯„ä¼°å•ä¸ªè§†é¢‘çš„è´¨é‡å’Œä¼ æ’­æ½œåŠ›"""
    from shared.domain import domain

    raw_text = v.get("rawtext", v["title"])
    
    # å¦‚æœæ—¶é•¿ä¸º0ï¼Œå°è¯•è¡¥å……è·å–
    duration = v.get("duration", 0)
    if duration <= 0:
        duration = get_video_duration(v["Video File"])
        v["duration"] = duration
    
    # è¯„åˆ†é€»è¾‘ï¼šæ—¶é•¿åŠ æƒ
    # æœ€ä½³åŒºé—´: 60s - 180s (1-3åˆ†é’Ÿ) -> æƒé‡åŠ åˆ†
    # æ¬¡ä½³åŒºé—´: 180s - 300s (3-5åˆ†é’Ÿ) -> æƒé‡ç¨ä½
    # å…¶ä»–: é™æƒ
    
    duration_score_bonus = 0
    if 60 <= duration <= 180:
        duration_score_bonus = 5  # é»„é‡‘æ—¶é•¿åŠ åˆ†
    elif 180 < duration <= 300:
        duration_score_bonus = 3  # æ¬¡ä½³æ—¶é•¿åŠ åˆ†
    elif duration > 0 and duration < 60:
         duration_score_bonus = -2 # å¤ªçŸ­
    
    # ä»é¢†åŸŸé…ç½®è¯»å–ç­›é€‰å‚æ•°
    categories, context = domain.get_screening_prompt()
    categories_str = "|".join(categories)

    prompt = f"""
ä½ æ˜¯ä¸“æ³¨äºä¸­æ–‡ç¤¾äº¤åª’ä½“ï¼ˆæŠ–éŸ³/å°çº¢ä¹¦/Bç«™ï¼‰çš„å†…å®¹è¿è¥ä¸“å®¶ã€‚è¯·è¯„ä¼°ä¸‹é¢æä¾›çš„å†…å®¹ç‰‡æ®µæ˜¯å¦å…·å¤‡æˆä¸ºçˆ†æ¬¾çš„æ½œåŠ›ï¼ˆæ— éœ€ç†ä¼šå‡ºå¤„ä¸é“¾æ¥ï¼‰ã€‚

ã€å†…å®¹æ¦‚è¦ã€‘{raw_text}s
ã€è§†é¢‘æ—¶é•¿ã€‘{duration:.1f}s

è¯„åˆ†æ ‡å‡†ï¼ˆæ»¡åˆ†30åˆ†ï¼‰ï¼š
- 25-30åˆ†ï¼šå¼ºçƒˆä¿¡æ¯å·®æˆ–é¢ è¦†æ€§ç»“è®º / é‡ç£…äººç‰©å¤±è¨€æˆ–ç½•è§è¡¨æ€ / æ”¿ç­–åè½¬æˆ–é‡å¤§å†²çª / å¤©ç„¶å…·å¤‡"è½¬å‘å†²åŠ¨"
- 15-24åˆ†ï¼šæœ‰å®è´¨ä¿¡æ¯é‡ï¼Œè§‚ç‚¹æ¸…æ™°ï¼Œç›®æ ‡å—ä¼—æœ‰å…±é¸£ï¼Œä½†ç¼ºä¹"ä¸€çœ¼çˆ†æ¬¾"ç‰¹è´¨
- 5-14åˆ†ï¼šä¿¡æ¯æ™®é€šï¼Œè§’åº¦å¹³åº¸ï¼Œéš¾ä»¥çªç ´æµé‡æ± 
- 0-4åˆ†ï¼šå†…å®¹æ— å…³ã€å¹¿å‘Šæ€§è´¨ã€æˆ–ä¿¡æ¯é‡æå°‘

ä»…è¿”å› JSONï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ï¼š
{{
  "score": <æ•´æ•° 0-30>,
  "title_cn": "æç‚¼20å­—ä»¥å†…çš„ä¸­æ–‡æ ‡é¢˜ï¼Œè®©äºº'ä¸€çœ¼çœ‹å‡ºä¸ºä»€ä¹ˆå€¼å¾—çœ‹'ï¼Œå¯ç”¨ç–‘é—®/åè½¬/æ•°å­—å¥å¼ï¼Œä¸¥ç¦å‡­ç©ºç¼–é€ ",
  "reason": "æ­¤å†…å®¹çš„æ ¸å¿ƒä¼ æ’­ä»·å€¼ç‚¹ï¼Œä»¥é€—å·åˆ†éš”çš„2-4ä¸ªå…³é”®è¯æˆ–çŸ­è¯­ï¼ˆä¾‹ï¼šç«‹åœºåè½¬,ç½•è§è¡¨æ€ï¼‰ï¼Œ20å­—ä»¥å†…",
  "category": "{categories_str}"
}}"""
    try:
        ai_res = ask_gpt(prompt, resp_type="json", log_title="video_screening")
        base_score = ai_res.get("score", 15)
        
        # åº”ç”¨æ—¶é•¿åŠ æƒ
        final_ai_score = base_score + duration_score_bonus
        v["AI Score"] = final_ai_score # ä¿å­˜å•ç‹¬çš„ AI è¯„åˆ†
        
        v["title"] = ai_res.get("title_cn", v["title"])
        v["AI Reason"] = ai_res.get("reason", "è¯„ä¼°å®Œæˆ")
        v["Category"] = ai_res.get("category", "Other")
        
        # åœ¨ç†ç”±ä¸­è¡¥å……æ—¶é•¿è¯„ä»·
        if duration_score_bonus > 0:
            v["AI Reason"] += f" [æ—¶é•¿é€‚å®œ {duration:.0f}s]"
        elif duration_score_bonus < 0:
            v["AI Reason"] += f" [æ—¶é•¿åçŸ­ {duration:.0f}s]"
            
    except Exception as e:
        console.print(f"  [dim yellow]âš ï¸ AI è¯„ä¼°å¤±è´¥ [{v.get('title', '')[:20]}]: {e}[/dim yellow]")
        v["AI Score"] = 10
        v["AI Reason"] = f"AI è¯„ä¼°å¤±è´¥: {str(e)[:60]}"
        v["Category"] = "Other"

    # ç»¼åˆæƒé‡ï¼šAI åˆ† * 1000 + çƒ­åº¦åŠ æƒ
    # ä½¿ç”¨å¤„ç†åçš„ AI Score è®¡ç®—æ€»åˆ†
    v["Score"] = v.get("AI Score", 0) * 1000 + (v.get("viewCount", 0) / 1000) + v.get("Reposts", 0)
    return v


def _ai_screening(videos: list) -> list:
    """å¹¶å‘ AI ç­›é€‰"""
    if not videos:
        return []

    console.print(
        f"\n[bold magenta]ğŸ¤– æ­£åœ¨å¯¹ {len(videos)} ä¸ªè§†é¢‘è¿›è¡Œ AI å¹¶å‘ç­›é€‰...[/bold magenta]"
    )

    with create_progress() as progress:
        task = progress.add_task("[cyan]è¯„ä¼°ä¸­...", total=len(videos))
        results = []
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(_evaluate_single, v) for v in videos]
            for future in futures:
                try:
                    results.append(future.result())
                except Exception as e:
                    console.print(f"  [dim red]âš ï¸ è¯„ä¼°ä»»åŠ¡å¼‚å¸¸: {e}[/dim red]")
                progress.advance(task)

    return results


# ==================== ä¸»å·¥ä½œæµ ====================

def pre_process_workflow():
    console.print(
        Panel.fit(
            "[bold blue]VideoLingo æ™ºèƒ½é‡‡é›†å·¥ä½œæµ[/bold blue]\n"
            "[dim]å…¨è‡ªåŠ¨é‡‡é›†ã€AI æ™ºèƒ½ç­›é€‰ã€å¢é‡æ›´æ–°ç³»ç»Ÿ[/dim]",
            border_style="cyan",
        )
    )

    visited = load_visited()

    # è·å–ç°æœ‰ Excel ä¸­çš„ URLï¼ˆç”¨äºå¢é‡å¯¹æ¯”ï¼‰
    existing_urls = set()
    if TASKS_EXCEL.exists():
        try:
            df_old = pd.read_excel(TASKS_EXCEL)
            if "Video File" in df_old.columns:
                existing_urls = set(df_old["Video File"].dropna().astype(str).tolist())
        except Exception as e:
            console.print(f"[yellow]âš ï¸ è¯»å–ç°æœ‰ Excel å¤±è´¥ï¼Œå…¨é‡æ›´æ–°: {e}[/yellow]")

    # ==================== 1. å¤šå¹³å°é‡‡é›† ====================

    all_fetched = []
    scrapers = _load_scrapers()

    console.print(f"\n[bold green]ğŸ“¡ å…±åŠ è½½ {len(scrapers)} ä¸ªé‡‡é›†å™¨[/bold green]")

    with console.status("[bold green]æ­£åœ¨æ‰§è¡Œå…¨å¹³å°å†…å®¹é‡‡é›†..."):
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰é‡‡é›†å™¨
        with ThreadPoolExecutor(max_workers=len(scrapers) + 2) as executor:
            # æäº¤ä»»åŠ¡
            future_to_scraper = {executor.submit(s.run): s for s in scrapers}
            
            for future in future_to_scraper:
                try:
                    results = future.result()
                    if results:
                        all_fetched.extend(results)
                except Exception as e:
                    s_name = future_to_scraper[future].name
                    console.print(f"[red]âŒ é‡‡é›†å™¨ {s_name} å¼‚å¸¸: {e}[/red]")

    if not all_fetched:
        console.print(
            Panel.fit(
                "[yellow]æœ¬æ¬¡å·¡æ£€æœªå‘ç°ä»»ä½•æ–°å†…å®¹ã€‚[/yellow]",
                title="é‡‡é›†æŠ¥å‘Š",
                border_style="yellow",
            )
        )
        return

    # ==================== 2. å¢é‡è¿‡æ»¤ ====================

    new_videos = []
    seen_in_batch = set()
    for v in all_fetched:
        url = str(v["Video File"])
        if url not in existing_urls and url not in seen_in_batch:
            new_videos.append(v)
            seen_in_batch.add(url)

    if not new_videos:
        console.print(
            Panel.fit(
                f"[cyan]å…±é‡‡é›† {len(all_fetched)} æ¡ï¼Œå‡ä¸ºå†å²è®°å½•ã€‚[/cyan]",
                title="å¢é‡ç»“æœ",
                border_style="blue",
            )
        )
        return

    console.print(
        f"\nâœ¨ [bold green]å¢é‡å‘ç° {len(new_videos)} æ¡å…¨æ–°å†…å®¹[/bold green] "
        f"[dim](å·²è¿‡æ»¤ {len(all_fetched) - len(new_videos)} æ¡é‡å¤)[/dim]"
    )

    # ==================== 3. AI ç­›é€‰ ====================

    screened = _ai_screening(new_videos)

    # ==================== 4. åˆå¹¶ä¿å­˜ ====================

    new_df = pd.DataFrame(screened)
    if TASKS_EXCEL.exists():
        try:
            old_df = pd.read_excel(TASKS_EXCEL)
            for col in STANDARD_COLUMNS:
                if col not in old_df.columns:
                    old_df[col] = ""
            final_df = pd.concat([old_df, new_df], ignore_index=True)
        except Exception:
            final_df = new_df
    else:
        final_df = new_df

    final_df.drop_duplicates(subset=["Video File"], keep="first", inplace=True)

    for col in STANDARD_COLUMNS:
        if col not in final_df.columns:
            final_df[col] = ""

    final_df = final_df[STANDARD_COLUMNS].copy()
    final_df["Score"] = pd.to_numeric(final_df["Score"], errors="coerce").fillna(0)
    final_df.sort_values(by="Score", ascending=False, inplace=True)

    TASKS_EXCEL.parent.mkdir(parents=True, exist_ok=True)
    final_df.to_excel(TASKS_EXCEL, index=False)

    # æ›´æ–°è®¿é—®è®°å½•
    for v in all_fetched:
        visited.add(v["Video File"])
    save_visited(visited)

    # ==================== 5. æ±‡æ€»è¾“å‡º ====================

    table = Table(title="âœ¨ é‡‡é›†ä¸ AI ç­›é€‰ä»»åŠ¡æ±‡æ€»")
    table.add_column("æ¥æº", style="cyan")
    table.add_column("æ—¶é•¿", style="yellow")
    table.add_column("åˆ†ç±»", style="blue")
    table.add_column("è¯„åˆ†", style="magenta")
    table.add_column("æ¨èæ ‡é¢˜", style="green")
    table.add_column("è¯„ä¼°ç†ç”±", style="dim")

    sorted_new = sorted(screened, key=lambda x: x["Score"], reverse=True)
    for v in sorted_new[:15]:
        display_score = f"{v['Score'] / 1000:.1f}"
        table.add_row(
            v["channel_name"],
            f"{v['duration']:.1f}s",
            v.get("Category", "N/A"),
            display_score,
            v["title"][:25] + "...",
            v.get("AI Reason", "")[:20] + "...",
        )

    console.print(table)
    console.print(
        f"\n[bold green]âœ… é‡‡é›†å·¥ä½œæµæ‰§è¡Œå®Œæ¯•ï¼æ–°å¢ {len(screened)} æ¡ä»»åŠ¡ã€‚[/bold green]"
    )


if __name__ == "__main__":
    pre_process_workflow()
