"""
ASR æ¨¡å— - è¯­éŸ³è¯†åˆ«å…¥å£

æ”¯æŒçš„ runtimeï¼ˆé€šè¿‡ config.yaml ä¸­ whisper.runtime é…ç½®ï¼‰:
    local       - æœ¬åœ° WhisperX æ¨¡å‹
    cloud       - 302 API (WhisperX äº‘ç«¯)
    elevenlabs  - ElevenLabs ASR API
    deepgram    - Deepgram Nova-3 API
"""

from core.utils import *
from core.asr_backend.demucs_vl import demucs_audio
from core.asr_backend.audio_preprocess import (
    process_transcription,
    convert_video_to_audio,
    split_audio,
    save_results,
    normalize_audio_volume,
)
from core._1_ytdlp import find_video_files
from core.utils.models import *


def _get_transcribe_fn(runtime: str):
    """æ ¹æ® runtime è¿”å›å¯¹åº”çš„è½¬å½•å‡½æ•°"""
    if runtime == "local":
        from core.asr_backend.whisperX_local import transcribe_audio as ts
        rprint("[cyan]ğŸ¤ Transcribing audio with local model...[/cyan]")
    elif runtime == "cloud":
        from core.asr_backend.whisperX_302 import transcribe_audio_302 as ts
        rprint("[cyan]ğŸ¤ Transcribing audio with 302 API...[/cyan]")
    elif runtime == "elevenlabs":
        from core.asr_backend.elevenlabs_asr import transcribe_audio_elevenlabs as ts
        rprint("[cyan]ğŸ¤ Transcribing audio with ElevenLabs API...[/cyan]")
    elif runtime == "deepgram":
        from core.asr_backend.deepgram import transcribe_audio_deepgram as ts
        rprint("[bold magenta]ğŸš€ ä½¿ç”¨ Deepgram Nova-3 API (æé€Ÿæ¨¡å¼)...[/bold magenta]")
    else:
        raise ValueError(f"Unknown ASR runtime: {runtime}")
    return ts


@check_file_exists(_2_CLEANED_CHUNKS)
def transcribe():
    """
    æ‰§è¡Œè¯­éŸ³è¯†åˆ«ã€‚

    å†…éƒ¨é€»è¾‘:
        1. è§†é¢‘è½¬éŸ³é¢‘
        2. äººå£°åˆ†ç¦»ï¼ˆå¯é€‰ï¼Œç”± demucs é…ç½®é¡¹æ§åˆ¶ï¼‰
        3. æŒ‰æ—¶é—´æ®µåˆ†å‰²éŸ³é¢‘
        4. é€æ®µ ASR è½¬å½•
        5. åˆå¹¶ç»“æœå¹¶ä¿å­˜
    """
    # 1. è§†é¢‘ â†’ éŸ³é¢‘
    video_file = find_video_files()
    convert_video_to_audio(video_file)

    # 2. äººå£°åˆ†ç¦»ï¼ˆå¯é€‰ï¼‰
    if load_key("demucs"):
        demucs_audio()
        vocal_audio = normalize_audio_volume(_VOCAL_AUDIO_FILE, _VOCAL_AUDIO_FILE, format="mp3")
    else:
        vocal_audio = _RAW_AUDIO_FILE

    # 3. åˆ†å‰²éŸ³é¢‘ç‰‡æ®µ
    segments = split_audio(_RAW_AUDIO_FILE)

    # 4. é€æ®µè½¬å½•
    runtime = load_key("whisper.runtime")
    ts = _get_transcribe_fn(runtime)

    all_results = []
    for start, end in segments:
        result = ts(_RAW_AUDIO_FILE, vocal_audio, start, end)
        all_results.append(result)

    # 5. åˆå¹¶ & ä¿å­˜
    combined_result = {"segments": []}
    for result in all_results:
        combined_result["segments"].extend(result["segments"])

    df = process_transcription(combined_result)
    save_results(df)


if __name__ == "__main__":
    transcribe()
