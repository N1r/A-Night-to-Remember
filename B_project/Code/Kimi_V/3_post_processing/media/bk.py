import json
import os
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime

import pandas as pd
import requests
from rapidfuzz import fuzz, process
from rich.console import Console


# ==================== é…ç½®åŒº ====================
def _load_config() -> tuple[str, str, str, bool]:
    """ä» configs/config.yaml è¯»å– API é…ç½®ï¼Œä¸é¡¹ç›®å…¶ä»–æ¨¡å—ä¿æŒä¸€è‡´"""
    try:
        import yaml
        cfg_path = Path(__file__).parent.parent.parent / "configs" / "config.yaml"
        with open(cfg_path, encoding="utf-8") as f:
            cfg = yaml.safe_load(f)
        
        api = cfg.get("api", {})
        return (
            api.get("key", ""),
            api.get("base_url", "https://api.longcat.chat/openai"),
            api.get("model", "LongCat-Flash-Lite"),
            bool(api.get("llm_support_json", False)),
        )
    except Exception:
        return (
            os.environ.get("LONGCAT_API_KEY", ""),
            "https://api.longcat.chat/openai",
            "LongCat-Flash-Lite",
            False,
        )

API_KEY, API_BASE_URL, API_MODEL, API_SUPPORT_JSON = _load_config()

PROJECT_ROOT = Path(__file__).parent.parent.parent.absolute()
READY_DIR = PROJECT_ROOT / "storage" / "ready_to_publish"
TASKS_EXCEL = PROJECT_ROOT / "storage" / "tasks" / "tasks_setting.xlsx"

console = Console()

# ==================== Excel ç¼“å­˜ï¼ˆæ‰¹é‡å¤„ç†æ—¶é¿å…é‡å¤ I/Oï¼‰====================

_excel_cache: dict = {"mtime": None, "df": None}


def _load_excel_df():
    """æ‡’åŠ è½½å¹¶ç¼“å­˜ Excel DataFrameï¼Œæ–‡ä»¶å˜æ›´æ—¶è‡ªåŠ¨å¤±æ•ˆ"""
    if not TASKS_EXCEL.exists():
        return None
    try:
        mtime = TASKS_EXCEL.stat().st_mtime
        if _excel_cache["mtime"] == mtime and _excel_cache["df"] is not None:
            return _excel_cache["df"]
        df = pd.read_excel(TASKS_EXCEL)
        _excel_cache.update({"mtime": mtime, "df": df})
        return df
    except Exception as e:
        console.log(f"[red]âŒ è¯»å– Excel å¤±è´¥: {e}[/red]")
        return None

# ==================== å·¥å…·å‡½æ•° ====================

def _extract_json(text: str) -> dict:
    """ä» AI å“åº”ä¸­æå– JSONï¼ŒæŒ‰ä¼˜å…ˆçº§å°è¯•ä¸‰ç§æ–¹å¼"""
    if not text:
        return {}
    # 1. ç›´æ¥è§£æ
    try:
        return json.loads(text.strip())
    except Exception:
        pass
    # 2. æå– {...} å—ï¼ˆä¿ç•™æ¢è¡Œï¼Œä¸åš replaceï¼‰
    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except Exception:
            pass
    # 3. å»é™¤ markdown ä»£ç å›´æ åå†è§£æ
    stripped = re.sub(r'^```(?:json)?\s*', '', text.strip(), flags=re.IGNORECASE)
    stripped = re.sub(r'\s*```$', '', stripped.strip())
    try:
        return json.loads(stripped)
    except Exception:
        console.log(f"[dim yellow]âš ï¸ JSON è§£æå½»åº•å¤±è´¥ï¼ŒåŸæ–‡èŠ‚é€‰: {text[:120]}[/dim yellow]")
        return {}


def _ask_gpt(system: str, user: str, temperature: float = 0.4) -> str:
    """
    è°ƒç”¨ LLM APIã€‚

    temperature é»˜è®¤ 0.4ï¼ˆä½åˆ›é€ æ€§ï¼‰ï¼š
    - é¿å… LLM è‡ªç”±å‘æŒ¥ã€ç¼–é€ å†…å®¹
    - ç¡®ä¿è¾“å‡ºå¿ å®äºæä¾›çš„ç´ æ
    """
    headers = {
        "Content-Type": "application/json",
    }
    # å…¼å®¹ API_KEY å¸¦/ä¸å¸¦ Bearer å‰ç¼€çš„æƒ…å†µ
    if API_KEY:
        key_str = API_KEY.strip()
        if key_str.lower().startswith("bearer "):
            headers["Authorization"] = key_str
        else:
            headers["Authorization"] = f"Bearer {key_str}"
    else:
        console.log("[yellow]âš ï¸ æœªæ£€æµ‹åˆ° LONGCAT API keyï¼ŒAuthorization å¤´ä¸ºç©º[/yellow]")
    payload = {
        "model": API_MODEL,
        "messages": [
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
        "temperature": temperature,
    }
    # å¦‚æœé…ç½®æ”¯æŒå¼ºç»“æ„åŒ– JSON è¾“å‡ºï¼Œå°è¯•ä½¿ç”¨ response_format ä»¥å‡å°‘è§£æé”™è¯¯
    if API_SUPPORT_JSON:
        try:
            payload["response_format"] = {"type": "json_object"}
        except Exception:
            pass
    try:
        response = requests.post(
            f"{API_BASE_URL}/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30,
        )
        try:
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"].strip()
        except requests.exceptions.HTTPError:
            # æ˜¾ç¤ºè¿”å›çš„çŠ¶æ€ç ä¸å“åº”ä½“ï¼Œä¾¿äºè¯Šæ–­ 401/403 ç­‰è®¤è¯é—®é¢˜
            console.log(f"[yellow]âš ï¸ API è¯·æ±‚å¤±è´¥ ({response.status_code}): {response.text}[/yellow]")
            return ""
    except Exception as e:
        console.log(f"[yellow]âš ï¸ API è¯·æ±‚å¤±è´¥: {e}[/yellow]")
        return ""


def _normalize_title(text: str) -> str:
    """æ¸…ç†å¹¶è§„èŒƒåŒ–æœç´¢æ–‡æœ¬"""
    if not text:
        return ""
    text = re.sub(r'https?://\S+', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    return re.sub(r'\s+', ' ', text).strip().lower()


def _ask_and_parse_json(system: str, user: str, example: str, temperature: float = 0.35, retries: int = 1, context: str = "") -> dict:
    """å‘æ¨¡å‹è¯·æ±‚å¹¶ä¸¥æ ¼è§£æ JSONï¼›å¤±è´¥æ—¶å¯çŸ­ä¿ƒé‡è¯•ä¸€æ¬¡ï¼Œè¦æ±‚æ¨¡å‹åªè¿”å›ç¬¦åˆç¤ºä¾‹çš„ JSONã€‚"""
    # é¦–æ¬¡å°è¯•
    res = _ask_gpt(system, user, temperature)
    parsed = _extract_json(res)
    # è‹¥é¦–æ¬¡å“åº”æ— æ³•è§£æä¸º JSONï¼Œç«‹å³è®°å½•ä»¥ä¾¿æ’æŸ¥ï¼ˆå³ä¾¿åç»­é‡è¯•æˆåŠŸä¹Ÿä¿ç•™åˆå§‹å“åº”ï¼‰
    if not parsed:
        try:
            dbg_dir = PROJECT_ROOT / "output" / "debug"
            dbg_dir.mkdir(parents=True, exist_ok=True)
            inter_path = dbg_dir / "metadata_ai_intermediate.log"
            with open(inter_path, "a", encoding="utf-8") as lf:
                lf.write("\n---\n")
                lf.write(f"time: {datetime.now().isoformat()}\n")
                lf.write(f"context: {context}\n")
                lf.write("initial_response:\n")
                lf.write((res or "") + "\n")
        except Exception:
            pass
    if parsed:
        return parsed

    # é‡è¯•ï¼šç®€çŸ­å¼ºçº¦æŸæŒ‡ä»¤ï¼Œè¦æ±‚ä»…è¿”å›ä¸¥æ ¼ JSON
    responses = [res]
    for _ in range(retries):
        retry_note = (
            "è¯·ä¸¥æ ¼åªè¿”å›æœ‰æ•ˆçš„ JSONï¼Œç»å¯¹ä¸è¦å¸¦è§£é‡Šæ–‡å­—æˆ–ä»£ç å—ï¼Œ" 
            f"å¦‚æœä¸èƒ½åˆ™è¿”å› {example} çš„åŒç»“æ„ç©ºå€¼ã€‚ç¤ºä¾‹ï¼š{example}"
        )
        combined = user + "\n\n" + retry_note
        res2 = _ask_gpt(system, combined, temperature)
        responses.append(res2)
        parsed = _extract_json(res2)
        if parsed:
            return parsed
    # è®°å½•å¤±è´¥çš„åŸå§‹å“åº”ä»¥ä¾¿æ’æŸ¥
    try:
        dbg_dir = PROJECT_ROOT / "output" / "debug"
        dbg_dir.mkdir(parents=True, exist_ok=True)
        log_path = dbg_dir / "metadata_ai_failures.log"
        with open(log_path, "a", encoding="utf-8") as lf:
            lf.write("\n---\n")
            lf.write(f"time: {datetime.now().isoformat()}\n")
            lf.write(f"context: {context}\n")
            lf.write("system_prompt:\n" + system.replace('\n', '\n') + "\n")
            lf.write("user_prompt:\n" + user.replace('\n', '\n') + "\n")
            for i, r in enumerate(responses):
                lf.write(f"response_{i+1}:\n")
                lf.write((r or "") + "\n")
    except Exception:
        pass

    return {}


def _read_srt_text(srt_path: Path, max_chars: int = 3000) -> str:
    """
    è¯»å– SRT æ–‡ä»¶ï¼Œè¿‡æ»¤åºå·å’Œæ—¶é—´è½´ï¼Œä»…ä¿ç•™å­—å¹•æ–‡æœ¬ã€‚

    max_chars: æœ€å¤šè¿”å›çš„å­—ç¬¦æ•°ï¼ˆä¿ç•™è¶³å¤Ÿå¤šä»¥è¦†ç›–è§†é¢‘æ ¸å¿ƒå†…å®¹ï¼‰
    """
    if not srt_path.exists():
        return ""
    try:
        content = srt_path.read_text(encoding='utf-8-sig', errors='ignore')
    except OSError:
        return ""
    timestamp_pattern = re.compile(r'\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}')
    lines = [
        line.strip()
        for line in content.splitlines()
        if line.strip() and not line.strip().isdigit() and not timestamp_pattern.match(line.strip())
    ]
    return "\n".join(lines)[:max_chars]


# ==================== è¿è¥å·¥å…· ====================

def clean_tag(text: str) -> str:
    """æ¸…ç†è¯é¢˜æ ‡ç­¾ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡å’Œä¸‹åˆ’çº¿"""
    if not text:
        return ""
    return re.sub(r'[^\w\u4e00-\u9fa5]', '', text).strip()


def _build_fallback_tags(info: dict) -> list:
    """ä» category/topics å­—æ®µæ´¾ç”Ÿå…œåº• tagsï¼Œé¿å…ç¡¬ç¼–ç æ³›åŒ–è¯"""
    tags = []
    category = info.get("category", "")
    if category and category not in ("æœªåˆ†ç±»", "International", ""):
        tags.append(clean_tag(category[:10]))
    for t in info.get("topics", [])[:2]:
        t = t.strip()
        cleaned = clean_tag(t)
        if cleaned and len(cleaned) <= 10:
            tags.append(cleaned)
    tags = [t for t in dict.fromkeys(tags) if t]  # å»é‡ä¿åº
    return tags[:3] if tags else ["çƒ­é—¨ä¿¡æ¯"]


def clean_tags_in_text(text: str) -> str:
    """è¯†åˆ«æ–‡æœ¬ä¸­çš„ #è¯é¢˜ å¹¶æ¸…ç†å…¶ä¸­çš„éæ³•å­—ç¬¦ï¼Œæ¸…ç†åä¸ºç©ºåˆ™ç§»é™¤"""
    if not text:
        return ""
    def _repl(match):
        cleaned = clean_tag(match.group(1))
        return f"#{cleaned}" if cleaned else ""
    return re.sub(r'#([^\s#]+)', _repl, text)


# ==================== Excel ä¿¡æ¯æå– ====================

def get_excel_info(folder_name: str) -> dict:
    """ä» Excel ä¸­é€šè¿‡æ¨¡ç³ŠåŒ¹é…æŸ¥æ‰¾ä¿¡æ¯ï¼Œä¼˜å…ˆåŒ¹é…ä¸­æ–‡æ ‡é¢˜åˆ—"""
    df = _load_excel_df()
    if df is None:
        return {}
    try:
        norm_folder = _normalize_title(folder_name)

        # 1. ä¼˜å…ˆåŒ¹é… 'title' åˆ—ï¼ˆStep 1 ç”Ÿæˆçš„ä¸­æ–‡æ ‡é¢˜ï¼‰
        if 'title' in df.columns:
            titles = df['title'].astype(str).tolist()
            norm_titles = [_normalize_title(t) for t in titles]
            match = process.extractOne(norm_folder, norm_titles, scorer=fuzz.ratio)
            if match and match[1] > 85:
                return _parse_row(df.iloc[norm_titles.index(match[0])])

        # 2. å…œåº•åŒ¹é… 'rawtext' åˆ—ï¼ˆåŸå§‹æ¨æ–‡/æ ‡é¢˜ï¼‰
        if 'rawtext' in df.columns:
            raws = df['rawtext'].astype(str).tolist()
            norm_raws = [_normalize_title(r) for r in raws]
            match = process.extractOne(norm_folder, norm_raws, scorer=fuzz.token_set_ratio)
            if match and match[1] > 80:
                return _parse_row(df.iloc[norm_raws.index(match[0])])

    except Exception as e:
        console.log(f"[red]âŒ åŒ¹é… Excel å¤±è´¥: {e}[/red]")
    return {}


def _parse_row(row) -> dict:
    """è§£æ Excel è¡Œæ•°æ®ä¸ºæ ‡å‡†å­—å…¸"""
    return {
        "title": str(row.get('title', '')),
        "summary": str(row.get('rawtext', '')),           # åŸå§‹æ¨æ–‡/æè¿°
        "category": str(row.get('Category', '')),
        "topics": str(row.get('AI Reason', '')).split(',') if row.get('AI Reason') else [],
        "translated_text": str(row.get('translated_text', '')),  # æ¨æ–‡ä¸­æ–‡ç¿»è¯‘
        "channel": str(row.get('channel_name', '')),
        "original_link": str(row.get('Video File', '')),
        "publish_date": str(row.get('Publish Date', '')),
    }


# ==================== å†…å®¹ä¸Šä¸‹æ–‡æ„å»º ====================

def _build_content_context(title: str, info: dict, srt_text: str) -> str:
    """
    æ„å»ºæä¾›ç»™æ‰€æœ‰å¹³å° prompt çš„ç»Ÿä¸€å†…å®¹ä¸Šä¸‹æ–‡ã€‚

    å°†æ‰€æœ‰å¯è·å–çš„åŸå§‹ä¿¡æ¯èšåˆï¼Œè®© LLM æœ‰è¶³å¤Ÿçš„äº‹å®ä¾æ®ï¼Œ
    å‡å°‘å› ä¿¡æ¯ä¸è¶³å¯¼è‡´çš„è‡ªç”±å‘æŒ¥å’Œå†…å®¹æé€ ã€‚
    """
    parts = []
    # if info.get("channel"):
    #     parts.append(f"ã€æ¥æºé¢‘é“ã€‘{info['channel']}")
    # if info.get("original_link"):
    #     parts.append(f"ã€åŸå§‹é“¾æ¥ã€‘{info['original_link']}")
    if title:
        parts.append(f"ã€ä¸­æ–‡æ ‡é¢˜ã€‘{title}")
    if info.get("summary"):
        parts.append(f"ã€åŸå§‹æ¨æ–‡/æè¿°ï¼ˆè‹±æ–‡ï¼‰ã€‘{info['summary']}")
    if info.get("translated_text"):
        parts.append(f"ã€ä¸­æ–‡ç¿»è¯‘ã€‘{info['translated_text']}")
    #if info.get("category"):
    #    parts.append(f"ã€å†…å®¹åˆ†ç±»ã€‘{info['category']}")
    # if info.get("topics"):
    #     topics = info['topics']
    #     topics_str = "ã€".join(t.strip() for t in topics if t.strip()) if isinstance(topics, list) else str(topics)
    #     if topics_str:
    #         parts.append(f"ã€æ”¶å½•ç†ç”±ã€‘{topics_str}")
    if srt_text:
        parts.append(f"ã€å…¨æ–‡ã€‘\n{srt_text}")

    context = "\n\n".join(parts)

    # æŠ—å¹»è§‰åŠå®‰å…¨åˆè§„çº¦æŸï¼šæ˜ç¡®è¦æ±‚ LLM ä¸å¾—ç¼–é€ å†…å®¹ï¼Œå¹¶è§„é¿æ•æ„Ÿè¯
    constraint = (
        "\n\nã€ä¸¥æ ¼çº¦æŸã€‘ä»¥ä¸Šæ˜¯è§†é¢‘çš„å…¨éƒ¨å·²çŸ¥ä¿¡æ¯ã€‚"
        "åªèƒ½åŸºäºä»¥ä¸Šå®é™…å†…å®¹è¿›è¡Œæç‚¼å’Œæ”¹å†™ï¼Œä¸¥ç¦ç¼–é€ è§†é¢‘ä¸­æœªå‡ºç°çš„äº‹ä»¶ã€äººç‰©ã€æ•°æ®æˆ–è§‚ç‚¹ã€‚"
    )
    return context + constraint


# ==================== å„å¹³å°å…ƒæ•°æ®ç”Ÿæˆå™¨ï¼ˆç‹¬ç«‹å‡½æ•°ï¼Œæ”¯æŒå¹¶å‘ï¼‰====================

def _gen_xhs_data(content_ctx: str, folder_name: str, info: dict, title: str) -> dict:
    """ç”Ÿæˆå°çº¢ä¹¦å¹³å°å…ƒæ•°æ®"""
    xhs_system = """
ä½ æ˜¯å°çº¢ä¹¦ä¼˜è´¨å†…å®¹åˆ›ä½œè€…ï¼Œæ“…é•¿ç”¨ç®€æ´ã€ä¸“ä¸šçš„è¯­è¨€æ‹†è§£è§†é¢‘å†…å®¹ï¼Œæå–æ ¸å¿ƒä»·å€¼å¹¶å¼•å‘è¯»è€…å…±é¸£ã€‚
åŸºäºæ‰€æä¾›çš„è§†é¢‘ç´ æï¼Œç”Ÿæˆä¸€æ¡ä¿¡æ¯é‡å……è¶³ã€è¡¨è¾¾æ¸…æ™°çš„ç¬”è®°æ–‡æ¡ˆï¼Œæ ¹æ®è§†é¢‘å†…å®¹è‡ªåŠ¨åŒ¹é…åˆé€‚çš„å‚ç±»é£æ ¼ã€‚

ä¸¥æ ¼è¦æ±‚ï¼ˆè¿ååˆ™è§†ä¸ºæ— æ•ˆè¾“å‡ºï¼‰ï¼š
1) ä»…è¿”å›åˆæ³• JSON å¯¹è±¡ï¼Œç»å¯¹ç¦æ­¢ Markdown ä»£ç å—æˆ–ä»»ä½•è§£é‡Šæ–‡å­—ã€‚
2) JSON ç»“æ„ï¼š{"title":"æ ‡é¢˜(15-20å­—)","desc":"æ­£æ–‡(150-200å­—)"}
3) ã€æ ‡é¢˜è§„èŒƒã€‘15-20å­—ï¼›æç‚¼è§†é¢‘æœ€æ ¸å¿ƒçš„äº®ç‚¹æˆ–æ‚¬å¿µï¼›
   å¯ç”¨ç–‘é—®å¥æˆ–æ•°å­—å½’çº³å¥å¼ï¼ˆå¦‚"å…³é”®3ç‚¹"ï¼‰ï¼›
   æ ¹æ®å†…å®¹å¯å«1ä¸ª Emojiï¼›ä¸¥ç¦å‡­ç©ºç¼–é€ ï¼Œä¸¥ç¦ä½¿ç”¨"éœ‡æƒŠ""å†…å¹•""çœŸç›¸"ç­‰ä½è´¨è¯æ±‡ã€‚
4) ã€æ­£æ–‡ç»“æ„ã€‘
   â‘  å†…å®¹æ¦‚è¿°ï¼š1-2å¥ç®€æ˜è¯´æ¸…æ¥šè¿™æœŸè§†é¢‘çš„æ ¸å¿ƒä¸»æ—¨
   â‘¡ æ ¸å¿ƒçœ‹ç‚¹/å¹²è´§ï¼š2-3æ®µåˆ†ç‚¹å±•å¼€ï¼Œç”¨"ğŸ“Œ""ğŸ”‘"ç­‰ç¬¦å·è¾…åŠ©é˜…è¯»
   â‘¢ å‡å/æ„Ÿæƒ³ï¼š1å¥æ€»ç»“æˆ–å‡åä¸»é¢˜
   â‘£ äº’åŠ¨ç»“å°¾ï¼šä¸€å¥å¼€æ”¾å¼é—®é¢˜é‚€è¯·è¯»è€…è®¨è®º
   â‘¤ è¯é¢˜æ ‡ç­¾ï¼š3-5ä¸ªï¼Œæ ¼å¼"#è¯é¢˜"ï¼Œ1-2ä¸ªå®½æ³›çš„æµé‡è¯é¢˜ + 2-3ä¸ªç²¾å‡†å‚ç±»è¯é¢˜
5) ä¸¥ç¦æé€ äº‹å®ï¼›æ‰€æœ‰å†…å®¹é¡»å¿ å®äºæä¾›çš„ç´ æã€‚
6) æ— æ³•è§£ææ—¶ï¼Œä»…è¿”å› {"error":"NO_JSON"}ã€‚
7) æ³¨æ„ï¼šä¸è¦åŒ…å«æ¢è¡Œç¬¦ï¼ˆ\nï¼‰æˆ–å…¶ä»–æœªè½¬ä¹‰å­—ç¬¦ï¼Œåªè¿”å›å•è¡Œç´§å‡‘çš„ JSONã€‚

ç¤ºä¾‹è¾“å‡ºï¼š
{"title":"è¿™ä¸ªè®¾è®¡å¤ªå¦™äº†ï¼å®ç”¨ä¸ç¾å­¦çš„å®Œç¾ç»“åˆğŸ“‹","desc":"æœ€è¿‘å‘ç°äº†ä¸€ä¸ªæå…¶å·§å¦™çš„è®¾è®¡ç†å¿µï¼Œå®ƒå°†æ—¥å¸¸å®ç”¨æ€§ä¸æç®€ç¾å­¦èåˆå¾—æ°åˆ°å¥½å¤„ã€‚\n\nğŸ“Œ æ ¸å¿ƒäº®ç‚¹ï¼šæ‰“ç ´äº†ä¼ ç»Ÿå¸ƒå±€çš„å±€é™ï¼Œè®©ç©ºé—´åˆ©ç”¨ç‡ç¿»å€\nğŸ”‘ ç»†èŠ‚ä¹‹å¤„ï¼šæè´¨çš„é€‰æ‹©å…¼é¡¾äº†è€ç”¨ä¸è§¦æ„Ÿ\n\nå¥½çš„è®¾è®¡ä¸ä»…è§£å†³é—®é¢˜ï¼Œè¿˜èƒ½æå‡ç”Ÿæ´»å“è´¨ã€‚ä½ å–œæ¬¢è¿™ä¸ªè®¾è®¡çš„å“ªä¸€éƒ¨åˆ†ï¼Ÿè¯„è®ºåŒºæ¥èŠï½\n\n#è®¾è®¡ç¾å­¦ #å®ç”¨å¹²è´§ #å¥½ç‰©åˆ†äº« #ç”Ÿæ´»æ–¹å¼ #çµæ„Ÿ"}
"""
    xhs_example = '{"title":"ğŸ¤” æ ‡é¢˜ç¤ºä¾‹","desc":"è¿™æ˜¯ä¸€ä¸ª 150 å­—å·¦å³çš„ç¤ºä¾‹æè¿°... #æ ‡ç­¾1 #æ ‡ç­¾2"}'
    data = _ask_and_parse_json(xhs_system, content_ctx, xhs_example, temperature=0.35, retries=1, context=f"{folder_name} - xhs")
    if data:
        data["desc"] = clean_tags_in_text(data.get("desc", ""))
    else:
        console.log("[yellow]âš ï¸ XHS AI è§£æå¤±è´¥ï¼ˆå·²è®°å½•è‡³ output/debugï¼‰ï¼Œä½¿ç”¨å…œåº•å€¼[/yellow]")
        data = {"title": f"ğŸ¤” {title[:18]}", "desc": f"{info.get('summary') or title}\n\n#çƒ­é—¨èµ„è®¯ #åˆ†äº«"}
    return data


def _gen_dy_data(content_ctx: str, folder_name: str, info: dict, title: str) -> dict:
    """ç”ŸæˆæŠ–éŸ³å¹³å°å…ƒæ•°æ®"""
    dy_system = """
ä½ æ˜¯æŠ–éŸ³ä¼˜è´¨çŸ­è§†é¢‘åˆ›ä½œè€…ï¼Œæ“…é•¿ç”¨ç®€æ˜æŠ“äººçš„è¯­è¨€æç‚¼è§†é¢‘ç²¾åï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€ŸæŠ“ä½æœ€å…·å¸å¼•åŠ›çš„çœ‹ç‚¹ã€‚

ä¸¥æ ¼è¦æ±‚ï¼š
1) ä»…è¿”å›ä¸¥æ ¼ JSONï¼Œç¦æ­¢ä»»ä½•é¢å¤–æ–‡å­—æˆ–ä»£ç å—ã€‚
2) JSON ç»“æ„ï¼š{"title":"æ ‡é¢˜(15-25å­—)","tags":["æ ‡ç­¾1","æ ‡ç­¾2","æ ‡ç­¾3"]}ï¼Œtags 3-5ä¸ªã€‚
3) ã€æ ‡é¢˜è§„èŒƒã€‘15-25å­—ï¼›ç›´æ¥ç‚¹å‡ºè§†é¢‘ä¸­æœ€å…·å†²çªæ„Ÿã€ä¿¡æ¯é‡æˆ–æƒ…ç»ªå…±é¸£çš„æ ¸å¿ƒï¼›
   å¯ç”¨é—®å¥æˆ–æ•°å­—å½’çº³å¥å¼ï¼›æ ¹æ®å†…å®¹çµæ´»è°ƒæ•´è¯­æ°”ï¼›
   å¯ç”¨"ï¼Ÿ""ï¼"å¢å¼ºè¯­æ°”ï¼Œä½†ä¸è¿‡åº¦å †å ï¼›
   ä¸¥ç¦åºŸè¯å¼€å¤´ï¼ˆ"ä»Šå¤©æˆ‘ä»¬æ¥èŠ""å¤§å®¶å¥½"ï¼‰ï¼›ä¸¥ç¦ç¼–é€ ï¼›ä¸¥ç¦ä½¿ç”¨"å†…å¹•""çœŸç›¸""éœ‡æƒŠ"ç­‰ä½è´¨è¯æ±‡ã€‚
4) ã€æ ‡ç­¾è§„èŒƒã€‘å‰1-2ä¸ªç”¨è¡Œä¸š/é¢†åŸŸçš„å¤§æµé‡è¯é¢˜ï¼›å2-3ä¸ªç”¨ç²¾å‡†æè¿°è§†é¢‘æ ¸å¿ƒå®ä½“/æ¦‚å¿µçš„è¯é¢˜ã€‚
5) è‹¥æ— æ³•ç”Ÿæˆï¼Œè¿”å› {"error":"NO_JSON"}ã€‚
6) æ³¨æ„ï¼šä¸è¦åŒ…å«æ¢è¡Œç¬¦ï¼ˆ\nï¼‰æˆ–å…¶ä»–æœªè½¬ä¹‰å­—ç¬¦ï¼Œåªè¿”å›å•è¡Œç´§å‡‘çš„ JSONã€‚

ç¤ºä¾‹è¾“å‡ºï¼š
{"title":"è¿™ä¸€åˆ»å¤ªç»äº†ï¼3ä¸ªç»†èŠ‚å¸¦ä½ çœ‹æ‡‚èƒŒåçš„é€»è¾‘","tags":["çœ‹ç‚¹è§£æ","ç»†èŠ‚è§£è¯»","çƒ­ç‚¹äº‹ä»¶","çŸ¥è¯†å¹²è´§"]}
"""
    dy_example = '{"title":"æ ‡é¢˜ç¤ºä¾‹","tags":["å¹²è´§","çœ‹ç‚¹","æ·±åº¦"]}'
    data = _ask_and_parse_json(dy_system, content_ctx, dy_example, temperature=0.3, retries=1, context=f"{folder_name} - dy")
    if data:
        data["tags"] = [clean_tag(t) for t in data.get("tags", []) if clean_tag(t)]
    else:
        console.log("[yellow]âš ï¸ DY AI è§£æå¤±è´¥[/yellow]")
        data = {"title": title[:20], "tags": _build_fallback_tags(info)}
    return data


def _gen_bili_data(content_ctx: str, folder_name: str, info: dict, title: str) -> dict:
    """ç”Ÿæˆ B ç«™å¹³å°å…ƒæ•°æ®"""
    bili_system = """
ä½ æ˜¯Bç«™èµ„æ·±ç¡¬æ ¸UPä¸»/ä¼˜è´¨å†…å®¹åˆ›ä½œè€…ï¼Œæ·±çŸ¥Bç«™ç”¨æˆ·åå¥½ï¼šä¿¡æ¯é‡å¤§ã€ç‹¬ç‰¹è§†è§’ã€æ³¨é‡å†…å®¹è´¨é‡ã€çƒ­çˆ±åœ¨è¯„è®ºåŒºæ·±åº¦äº¤æµã€‚

ä¸¥æ ¼è¦æ±‚ï¼š
1) ä»…è¿”å›ä¸¥æ ¼ JSONï¼Œç¦æ­¢å¤šä½™è¯´æ˜æˆ–ä»£ç å—ã€‚
2) JSON ç»“æ„ï¼š{"title":"æ ‡é¢˜(30-80å­—)","desc":"è§†é¢‘ç®€ä»‹(3-5å¥è¯)"}ã€‚
3) ã€æ ‡é¢˜è§„èŒƒã€‘æ ¹æ®æƒ…å†µå¯é€‰[å†…å®¹æ ‡è®°] + æ ¸å¿ƒä¸»ä½“ + æœ€å¼ºçœ‹ç‚¹ï¼ˆç”¨"ï¼"æˆ–"ï¼Ÿ"æ”¶å°¾ï¼‰ã€‚
   æ ‡è®°å¯é€‰ï¼šã€ä¸­æ–‡å­—å¹•ã€‘ã€ç¡¬æ ¸è§£æã€‘ã€é¦–å‘ã€‘ç­‰ï¼ˆè§†è§†é¢‘æ€§è´¨è€Œå®šï¼‰ï¼›
   æ ¸å¿ƒä¸»ä½“è¦ç®€æ´ï¼Œçœ‹ç‚¹éœ€ç‚¹å‡ºæœ€æœ‰ä»·å€¼ã€æœ€æœ‰è¶£æˆ–æœ€å…·ä¿¡æ¯é‡çš„éƒ¨åˆ†ã€‚80å­—ä»¥å†…ã€‚
   å‚è€ƒæ ¼å¼ï¼š"ã€ç¡¬æ ¸è§£æã€‘å¹²è´§æ‹‰æ»¡ï¼å…³äºè¿™é¡¹æŠ€æœ¯çš„3ä¸ªæ ¸å¿ƒè¯¯åŒºï¼Œä¸€æ¬¡è®²é€ï¼"
4) ã€ç®€ä»‹è§„èŒƒï¼ˆ4æ®µå¼ï¼‰ã€‘
   â‘  èƒŒæ™¯ä»‹ç»ï¼šä¸€å¥è¯ç®€æ˜è¯´æ¸…æ¥šè¿™æ®µè§†é¢‘åˆ†äº«çš„å…·ä½“å†…å®¹æ˜¯ä»€ä¹ˆ
   â‘¡ åˆ¶ä½œè¯´æ˜ï¼šå¦‚"å†…å®¹å·²ç»è¿‡ç²¾å¿ƒæ•´ç†ç¼–è¯‘/ç¿»è¯‘ï¼Œé‡ç‚¹å·²æå–"ï¼ˆçœ‹å®é™…æƒ…å†µå†™ï¼‰
   â‘¢ æ ¸å¿ƒçœ‹ç‚¹ï¼šç‚¹æ˜æœ€ç²¾å½©çš„1-2ä¸ªäº®ç‚¹ã€å¹²è´§æˆ–çœ‹ç‚¹
   â‘£ äº’åŠ¨å¼•å¯¼ï¼ˆéšæœºå˜ä½“ï¼‰ï¼š
      Â· "è§‰å¾—è§†é¢‘æœ‰ç”¨çš„é¡ºæ‰‹ç‚¹ä¸ªèµæ”¯æŒä¸€ä¸‹ï¼"
      Â· "å¯¹äºè¿™ç‚¹å¤§å®¶æ€ä¹ˆçœ‹ï¼Ÿè¯„è®ºåŒºè§ï½"
      Â· "æ±‚ä¸ªä¸€é”®ä¸‰è¿ï¼Œè®©ä½ çš„é¦–é¡µå¤šäº›ç¡¬æ ¸å¹²è´§ï¼"
   ä¸¥ç¦æé€ äº‹å®ã€‚
5) è‹¥æ— æ³•è¾“å‡ºæœ‰æ•ˆ JSONï¼Œè¿”å› {"error":"NO_JSON"}ã€‚
6) æ³¨æ„ï¼šä¸è¦åŒ…å«æ¢è¡Œç¬¦ï¼ˆ\nï¼‰æˆ–å…¶ä»–æœªè½¬ä¹‰å­—ç¬¦ï¼Œåªè¿”å›å•è¡Œç´§å‡‘çš„ JSONã€‚

ç¤ºä¾‹è¾“å‡ºï¼š
{"title":"ã€ç¡¬æ ¸è§£æã€‘è¿™ä¹Ÿè®¸æ˜¯ä½ è§è¿‡æœ€æ¸…æ™°çš„åŸç†è§£æï¼å¸¦ä½ é‡æ–°è®¤è¯†è¿™ä¸ªé¢†åŸŸ","desc":"æœ¬æœŸè§†é¢‘å¸¦å¤§å®¶æ·±å…¥äº†è§£è¯¥é¢†åŸŸçš„æœ€æ–°åŠ¨å‘ï¼Œæ ¸å¿ƒçœ‹ç‚¹åœ¨äºååŠæ®µå¯¹åº•å±‚é€»è¾‘çš„å‰–æï¼Œç›´æ¥ç‚¹å‡ºäº†é•¿æœŸä»¥æ¥çš„è®¤çŸ¥ç›²åŒºã€‚è§‰å¾—æœ‰æ”¶è·çš„æœ‹å‹ç‚¹ä¸ªèµæ”¯æŒä¸€ä¸‹ï¼"}
"""
    bili_example = '{"title":"ã€ç¡¬æ ¸è§£æã€‘ç¤ºä¾‹æ ‡é¢˜","desc":"ç¡¬æ ¸å¹²è´§è§£æï¼Œæ±‚ä¸€é”®ä¸‰è¿æ”¯æŒï¼"}'
    data = _ask_and_parse_json(bili_system, content_ctx, bili_example, temperature=0.35, retries=1, context=f"{folder_name} - bili")
    if data:
        data["desc"] = clean_tags_in_text(data.get("desc", ""))
    else:
        console.log("[yellow]âš ï¸ BILI AI è§£æå¤±è´¥[/yellow]")
        data = {
            "title": f"ã€åŒè¯­å­—å¹•ã€‘{title}",
            "desc": f"Antigravity å­—å¹•ç»„å‡ºå“ï¼Œæ±‚ä¸ªä¸€é”®ä¸‰è¿æ”¯æŒï¼\n{info.get('summary') or ''}",
        }
    return data


def _gen_ks_data(content_ctx: str, folder_name: str, info: dict, title: str) -> dict:
    """ç”Ÿæˆå¿«æ‰‹å¹³å°å…ƒæ•°æ®"""
    ks_system = """
ä½ æ˜¯å¿«æ‰‹å¤´éƒ¨çƒ­ç‚¹å†…å®¹åˆ›ä½œè€…ï¼Œç²‰ä¸ç”»åƒä¸‹æ²‰å¤§ä¼—ï¼Œä¿—ç§°"è€é“"ã€‚
ä½ çš„å†…å®¹é£æ ¼ï¼šæåº¦å£è¯­åŒ–ã€ç›´ç™½æ¥åœ°æ°”ã€æƒ…ç»ªé¥±æ»¡ï¼Œè®©äººä¸€çœ¼å°±æƒ³ç‚¹å¼€ã€çœ‹å®Œå°±æƒ³è½¬å‘ã€‚

ä¸¥æ ¼è¦æ±‚ï¼š
1) åªè¿”å›ä¸¥æ ¼ JSONï¼Œç¦æ­¢è§£é‡Šæ–‡å­—æˆ–ä»£ç å—ã€‚
2) JSON ç»“æ„ï¼š{"title":"æ ‡é¢˜(10-18å­—)","tags":["æ ‡ç­¾1","æ ‡ç­¾2","æ ‡ç­¾3"]}ï¼Œtags 3-5ä¸ªã€‚
3) ã€æ ‡é¢˜è§„èŒƒã€‘10-18å­—ï¼›æåº¦å£è¯­åŒ–ï¼Œå°±åƒåœ¨è·Ÿæœ‹å‹å” å—‘ï¼›
   å¯ç”¨å¼•å¯¼å¼ï¼ˆ"è¿™äº‹åˆå‡ºæ–°èŠ±æ ·äº†"ï¼‰æˆ–é—®å¥å¼ï¼ˆ"è€é“ä»¬é‡åˆ°è¿‡è¿™æƒ…å†µä¸"ï¼‰ï¼›
   å–„ç”¨æ¥åœ°æ°”è¯æ±‡ï¼š"è¿™""å’±""è€é“""æ•´""ç»äº†""å¤ªç‰›äº†""ç»™æ•´ä¸ä¼šäº†"ï¼›
   ä¸¥ç¦ç”Ÿç¡¬æ–‡ç»‰ç»‰çš„ä¹¦é¢è¯­ï¼›ä¸¥ç¦ç¼–é€ ã€‚
4) ã€æ ‡ç­¾è§„èŒƒã€‘æ··åˆå…¨ç½‘çƒ­é—¨åˆ†ç±»è¯ï¼ˆ1-2ä¸ªï¼Œæ‰©å¤§æ›å…‰ï¼‰+ ç²¾å‡†æè¿°è§†é¢‘çš„é€šä¿—è¯æ¡ï¼ˆ2-3ä¸ªï¼Œç²¾å‡†è§¦è¾¾ï¼‰ã€‚
   é£æ ¼æ¥åœ°æ°”ï¼Œç¬¦åˆå¿«æ‰‹ä¸»æµå—ä¼—æœç´¢ä¹ æƒ¯ã€‚
5) è‹¥æ— æ³•è¿”å›æœ‰æ•ˆ JSONï¼Œè¿”å› {"error":"NO_JSON"}ã€‚
6) æ³¨æ„ï¼šä¸è¦åŒ…å«æ¢è¡Œç¬¦ï¼ˆ\nï¼‰æˆ–å…¶ä»–æœªè½¬ä¹‰å­—ç¬¦ï¼Œåªè¿”å›å•è¡Œç´§å‡‘çš„ JSONã€‚

ç¤ºä¾‹è¾“å‡ºï¼š
{"title":"è¿™æ“ä½œçœŸæ˜¯ç»äº†è€é“ä»¬çœ‹çœ‹","tags":["ç”Ÿæ´»æ—¥å¸¸","æ¶¨çŸ¥è¯†","å¤ªç‰›äº†","ä½ æ•¢ä¿¡"]}
"""
    ks_example = '{"title":"æƒŠè®¶å¼æ ‡é¢˜è¿™äº‹ç»äº†","tags":["çƒ­ç‚¹","æ¶¨çŸ¥è¯†"]}'
    data = _ask_and_parse_json(ks_system, content_ctx, ks_example, temperature=0.3, retries=1, context=f"{folder_name} - ks")
    if data:
        data["tags"] = [clean_tag(t) for t in data.get("tags", []) if clean_tag(t)]
    else:
        console.log("[yellow]âš ï¸ KS AI è§£æå¤±è´¥[/yellow]")
        data = {"title": title[:18], "tags": _build_fallback_tags(info)}
    return data


def _generate_all_platforms(content_ctx: str, folder_name: str, info: dict, title: str) -> dict:
    """
    å¹¶å‘ä¸º 4 ä¸ªå¹³å°ç”Ÿæˆå…ƒæ•°æ®ï¼Œæ€»è€—æ—¶çº¦ç­‰äºæœ€æ…¢çš„å•ä¸ªå¹³å°ã€‚

    ä¸²è¡Œæ—¶çº¦ 27s/è§†é¢‘ â†’ å¹¶å‘åçº¦ 10s/è§†é¢‘ã€‚
    """
    tasks = {
        "xiaohongshu": lambda: _gen_xhs_data(content_ctx, folder_name, info, title),
        "douyin":       lambda: _gen_dy_data(content_ctx, folder_name, info, title),
        "bilibili":     lambda: _gen_bili_data(content_ctx, folder_name, info, title),
        "kuaishou":     lambda: _gen_ks_data(content_ctx, folder_name, info, title),
    }
    results: dict = {}
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {executor.submit(fn): key for key, fn in tasks.items()}
        for future in as_completed(futures):
            key = futures[future]
            try:
                results[key] = future.result()
            except Exception as e:
                console.log(f"[red]âŒ {key} å¹¶å‘ç”Ÿæˆå¤±è´¥: {e}[/red]")
                results[key] = {}
    return results


# ==================== æ ¸å¿ƒé€»è¾‘ ====================

def generate_metadata_for_folder(folder: Path):
    """ä¸ºå•ä¸ªç›®å½•ç”Ÿæˆ metadata.json"""
    meta_path = folder / "metadata.json"

    # è¯»å–å·²æœ‰å…ƒæ•°æ®
    existing_meta = {}
    if meta_path.exists():
        try:
            existing_meta = json.loads(meta_path.read_text(encoding='utf-8'))
        except Exception:
            pass

    # å››ä¸ªå¹³å°å‡å·²æœ‰å…ƒæ•°æ®æ—¶è·³è¿‡ï¼ˆèŠ‚çœ API é¢åº¦ï¼‰
    _target_platforms = ("douyin", "xiaohongshu", "bilibili", "kuaishou")
    if existing_meta and all(
        existing_meta.get("platforms", {}).get(p)
        for p in _target_platforms
    ):
        console.log(f"  â­ï¸ [dim]å…ƒæ•°æ®å·²å®Œæ•´ï¼Œè·³è¿‡: {folder.name}[/dim]")
        return

    # 1. ä» Excel è·å–åŸå§‹ä¿¡æ¯
    info = get_excel_info(folder.name)
    if info:
        console.log(f"  ğŸ“ [dim]Excel åŒ¹é…æˆåŠŸ: {info.get('title', '')[:30]}[/dim]")

    # 2. ç¡®å®šä¸­æ–‡æ ‡é¢˜
    title = existing_meta.get("translated_title") or info.get("title") or existing_meta.get("title") or folder.name

    # 3. è¯»å–å­—å¹•ï¼ˆä¼˜å…ˆ artifacts/ ç›®å½•ä¸‹ï¼Œå…¼å®¹æ—§è·¯å¾„ï¼‰
    srt_candidates = [
        folder / "artifacts" / "trans.srt",
        folder / "trans.srt",
        folder / "artifacts" / "output.srt",
    ]
    srt_text = ""
    for srt_path in srt_candidates:
        srt_text = _read_srt_text(srt_path, max_chars=3000)
        if srt_text:
            break

    # 4. æ„å»ºç»Ÿä¸€å†…å®¹ä¸Šä¸‹æ–‡
    content_ctx = _build_content_context(title, info, srt_text)
    console.log(f"ğŸ§  [cyan]æ­£åœ¨å¹¶å‘ç”Ÿæˆå¤šå¹³å°è¿è¥ç­–åˆ’: {title[:25]}...[/cyan]")

    # 5. å¹¶å‘è°ƒç”¨ 4 ä¸ªå¹³å°ç”Ÿæˆå™¨ï¼ˆä¸²è¡Œ ~27s â†’ å¹¶å‘ ~10sï¼‰
    platform_data = _generate_all_platforms(content_ctx, folder.name, info, title)
    xhs_data  = platform_data.get("xiaohongshu", {})
    dy_data   = platform_data.get("douyin", {})
    bili_data = platform_data.get("bilibili", {})
    ks_data   = platform_data.get("kuaishou", {})

    # 6. æ„å»ºæœ€ç»ˆå…ƒæ•°æ®
    summary = info.get("summary") or info.get("translated_text") or ""
    new_meta = {
        "title": folder.name,
        "translated_title": title,
        "summary": summary,
        "category": info.get("category", "æœªåˆ†ç±»"),
        "topics": info.get("topics", []),
        "channel": info.get("channel", "Unknown"),
        "original_link": info.get("original_link", ""),
        "publish_date": info.get("publish_date", ""),
        "generate_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "platforms": {
            "douyin": {
                "title": dy_data.get("title", title)[:30],
                "tags": dy_data.get("tags", ["çƒ­ç‚¹"]),
            },
            "xiaohongshu": {
                "title": xhs_data.get("title", title)[:30],
                "desc": xhs_data.get("desc", f"{title}\n\n{summary}"),
                "tags": [],  # æ ‡ç­¾å·²åŒ…å«åœ¨ desc ä¸­
            },
            "bilibili": {
                "title": bili_data.get("title", title)[:80],
                "desc": bili_data.get("desc", summary),
                "tags": ["å¹²è´§", "åˆ†äº«", "çƒ­ç‚¹çŸ¥è¯†"] + _build_fallback_tags(info)[:1],
            },
            "kuaishou": {
                "title": ks_data.get("title", title)[:18],
                "tags": ks_data.get("tags", ["çƒ­ç‚¹"]),
            },
        },
    }

    meta_path.write_text(json.dumps(new_meta, ensure_ascii=False, indent=2), encoding="utf-8")
    console.log(f"âœ… [green]å…ƒæ•°æ®ç”ŸæˆæˆåŠŸ:[/green] {folder.name} -> {title}")


def _clear_platforms(folders):
    """æ¸…é™¤å„æ–‡ä»¶å¤¹ metadata.json ä¸­çš„ platforms å—ï¼Œä½¿ç”Ÿæˆé€»è¾‘å¼ºåˆ¶é‡è·‘"""
    count = 0
    for folder in folders:
        meta_path = folder / "metadata.json"
        if not meta_path.exists():
            continue
        try:
            meta = json.loads(meta_path.read_text(encoding="utf-8"))
            if "platforms" in meta:
                meta.pop("platforms")
                meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")
                count += 1
        except Exception as e:
            console.log(f"  [yellow]âš ï¸ æ¸…é™¤ {folder.name} platforms å¤±è´¥: {e}[/yellow]")
    console.log(f"[cyan]ğŸ—‘ï¸ å·²æ¸…é™¤ {count} ä¸ªæ–‡ä»¶å¤¹çš„ platforms ç¼“å­˜[/cyan]")


def process_ready_dir(force: bool = False):
    """éå† ready_to_publish è¡¥å…¨å…ƒæ•°æ®

    Parameters
    ----------
    force : bool
        True æ—¶å…ˆæ¸…é™¤å„æ–‡ä»¶å¤¹ metadata.json ä¸­çš„ platforms å—ï¼Œå¼ºåˆ¶é‡æ–°ç”Ÿæˆ
    """
    if not READY_DIR.exists():
        console.log(f"[red]âŒ ç›®å½•ä¸å­˜åœ¨: {READY_DIR}[/red]")
        return

    folders = [f for f in READY_DIR.iterdir() if f.is_dir() and f.name not in ("done", "failed")]
    if not folders:
        console.log("[yellow]âš ï¸ æ— å¾…å¤„ç†ç›®å½•[/yellow]")
        return

    if force:
        _clear_platforms(folders)

    console.rule("[bold cyan]å…ƒæ•°æ®æ™ºèƒ½åŒ–è¡¥å…¨[/bold cyan]")
    for folder in folders:
        generate_metadata_for_folder(folder)


if __name__ == "__main__":
    import sys
    process_ready_dir(force="--force" in sys.argv or "-f" in sys.argv)